[K8S] Pháº§n 9: Service Monitor - CÃ¡ch giÃ¡m sÃ¡t á»©ng dá»¥ng trÃªn Prometheus má»™t cÃ¡ch nhanh chÃ³ng vÃ  hiá»‡u quáº£
=======================================================================================================

[MayFest2022](https://viblo.asia/tags/mayfest2022)[Reconnection](https://viblo.asia/tags/mayfest2022)

Lá»i tá»±a
=======

ChÃ o cÃ¡c báº¡n, sau bÃ i viáº¿t vá» Monitoring trÃªn Kubernetes sá»­ dá»¥ng Prometheus thÃ¬ cÃ³ ráº¥t nhiá»u báº¡n quan tÃ¢m tá»›i chá»§ Ä‘á» nÃ y. Äáº·c biá»‡t lÃ  cÃ³ má»™t khÃ¡i niá»‡m nhiá»u ngÆ°á»i nháº¯c tá»›i lÃ  "Service Monitoring". Váº­y nÃ³ lÃ  gÃ¬, vÃ  á»©ng dá»¥ng gÃ¬ cho viá»‡c Monitoring?

*ThÃ¬ trong bÃ i hÃ´m nay mÃ¬nh sáº½ giá»›i thiá»‡u nhiá»u hÆ¡n má»™t chÃºt vá» khÃ¡i niá»‡m "Service Monitor" nhÃ©!*

Giá»›i thiá»‡u
==========

Trong pháº§n trÆ°á»›c mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u vá» cÃ¡c thÃ nh pháº§n cá»§a monitoring gá»“m Alert Manager - Prometheus - Grafana (pháº§n 8:Â <https://viblo.asia/p/k8s-phan-8-monitoring-tren-kubernetes-cluster-dung-prometheus-va-grafana-Qbq5QRkEKD8>)

Má»¥c tiÃªu cá»§a bÃ i viáº¿t hÃ´m nay sáº½ giÃºp cÃ¡c báº¡n:

-   CÃ¡ch cáº¥u hÃ¬nh láº¥y metric má»™t á»©ng dá»¥ng báº±ng cáº¥u hÃ¬nh job trongÂ scrapeConfig
-   CÃ¡ch cáº¥u hÃ¬nh láº¥y metric má»™t á»©ng dá»¥ng báº±ng "Service Monitor"
-   CÃ¡ch cÃ i má»™t á»©ng dá»¥ng há»— trá»£ sáºµnÂ metricÂ vÃ Â serviceMonitor
-   CÃ¡ch trouble-shoot cÃ¡c váº¥n Ä‘á» phÃ¡t sinh trong khi cáº¥u hÃ¬nh --> ÄÃ¢y chÃ­nh lÃ  bÆ°á»›c quan trá»ng nháº¥t bá»Ÿi nhiá»u báº¡n cáº¥u hÃ¬nh ko cháº¡y Ä‘Æ°á»£c nhÆ°ng láº¡i ko biáº¿t lÃ m tháº¿ nÃ o Ä‘á»ƒ check xem mÃ¬nh sai á»Ÿ Ä‘Ã¢u vÃ  cáº§n sá»­a gÃ¬ rá»“i Ä‘i vÃ o ngÃµ cá»¥t, cháº£n náº£n vÃ  bá» cuá»™cÂ ![ğŸ˜ƒ](https://twemoji.maxcdn.com/v/14.0.2/72x72/1f603.png))

VÃ  Ä‘á»ƒ Ä‘i vÃ o tá»«ng bÆ°á»›c chi tiáº¿t thÃ¬ ta nÃªn cÃ³ cÃ¡i nhÃ¬n tá»•ng quan trÆ°á»›c Ä‘Ã£, ta cÃ³ kiáº¿n trÃºc nhÆ° sau:

![image.png](https://images.viblo.asia/e2e4da27-b5f2-4e8d-bd83-5fd9949b1885.png)

Luá»“ng giÃ¡m sÃ¡t Ä‘Æ°á»£c mÃ´ táº£ nhÆ° sau:

-   Prometheus láº¥y thÃ´ng tin metric tá»« cÃ¡c Ä‘á»‘i tÆ°á»£ng cáº§n giÃ¡m sÃ¡t. Minh táº¡m chia cÃ¡c Ä‘á»‘i tÆ°á»£ng nÃ y thÃ nh 2 loáº¡i:
    -   Má»™t loáº¡i há»— trá»£ expose metric tÆ°Æ¡ng thÃ­ch vá»›i Prometheus, nghÄ©a lÃ  nÃ³ cÃ³ sáºµn api cho viá»‡c láº¥y metric vÃ  ta chá»‰ cáº§n cáº¥u hÃ¬nh prometheus láº¥y metric tá»« Ä‘Ã³
    -   Má»™t loáº¡i khÃ´ng há»— trá»£ sáºµn metric mÃ  ta sáº½ pháº£i cÃ i thÃªm má»™t exporter (node-exporter lÃ  má»™t vÃ­ dá»¥ vá» exporter Ä‘á»ƒ láº¥y metric cá»§a node)
    -   Note: Viá»‡c prometheus láº¥y dá»¯ liá»‡u tá»« Ä‘á»‘i tÆ°á»£ng nÃ o Ä‘Æ°á»£c gá»i lÃ  cÃ¡c "job". CÃ¡c job nÃ y chá»©a thÃ´ng tin Ä‘á»‘i tÆ°á»£ng nÃ³ cáº§n láº¥y metric cÅ©ng nhÆ° cÃ¡ch láº¥y metric (táº§n suáº¥t, thá»i gian lÆ°u...). VÃ  Ä‘á»ƒ táº¡o cÃ¡c "job" nÃ y cÃ³ 2 cÃ¡ch:
        -   Cáº¥u hÃ¬nhÂ *scrape-config*Â cho prometheus: CÃ¡ch nÃ y mÃ¬nh cho lÃ  cÃ¡ch truyá»n thá»‘ng, vÃ  quáº£n lÃ½ sáº½ ráº¥t váº¥t váº£ náº¿u sá»‘ lÆ°á»£ng job lá»›n thÃ¬ file cáº¥u hÃ¬nh cá»§a báº¡n sáº½ phÃ¬nh to vÃ  dá»… máº¥t control @@. HÆ¡n ná»¯a má»—i láº§n update thÃ¬ báº¡n sáº½ pháº£i update cáº£ stack Ä‘á»ƒ nÃ³ update config má»›i
        -   Cáº¥u hÃ¬nhÂ *service monitor*: CÃ¡ch nÃ y sáº½ hiá»‡u quáº£ hÆ¡n vÃ¬ báº¡n sáº½ kiá»ƒm soÃ¡t tá»‘t hÆ¡n vá»›i tá»«ng Ä‘á»‘i tÆ°á»£ng giÃ¡m sÃ¡t sáº½ tÆ°Æ¡ng á»©ng má»™t file yaml cáº¥u hÃ¬nh riÃªng cho nÃ³. HÆ¡n ná»¯a khÃ´ng pháº£i Ä‘á»•i láº¡i cáº¥u hÃ¬nh cá»§a prometheus server (khÃ´ng cáº§n update stack)
-   Khi láº¥y Ä‘Æ°á»£c metric thÃ¬ nÃ³ sáº½ "lÃ m giÃ u" thÃ´ng tin cá»§a metric Ä‘Ã³, vÃ­ dá»¥ gÃ¡n thÃªm cÃ¡c label nhÆ°ng namespace, jobname, servicename.. Ä‘á»ƒ phÃ¢n loáº¡i sau nÃ y vÃ  ghi vÃ o database cá»§a Prometheus (lÃ m má»™t internal Time Series Database).
-   Prometheus Ä‘á»c cÃ¡c prometheus rule (lÃ  cÃ¡c hÃ m so sÃ¡nh giÃ¡ trá»‹ metric vá»›i cÃ¡c ngÆ°á»¡ng xÃ¡c Ä‘á»‹nh) Ä‘á»ƒ quyáº¿t Ä‘á»‹nh nhá»¯ng rule nÃ o cáº§n cáº£nh bÃ¡o (alert) Ä‘á»ƒ Ä‘áº©y vá» Alert Manager
-   Alert Manager sáº½ cÃ³ config riÃªng cá»§a nÃ³ Ä‘á»ƒ thá»±c hiá»‡n viá»‡c "phÃ¢n luá»“ng" cáº£nh bÃ¡o tá»›i cÃ¡c ngÆ°á»i nháº­n khÃ¡c nhau, viá»‡c nÃ y gá»i lÃ  "route". ThÃ´ng tin ngÆ°á»i nháº­n (gá»i lÃ  receiver) Ä‘Æ°á»£c cáº¥u hÃ¬nh á»Ÿ Alert Manager, há»— trá»£ khÃ¡ Ä‘a dáº¡ng tá»« email, slack, msteam, telegram...

Prometheus Job vÃ  Service monitor
=================================

Giá» mÃ¬nh sáº½ giá»›i thiá»‡u ká»¹ hÆ¡n vá» Prometheus Job vÃ  cÃ¡ch táº¡o job nÃ y báº±ng Service Monitor.

Prometheus Job
--------------

LÃ  Ä‘á»‘i tÆ°á»£ng lÆ°u thÃ´ng tin vá» cÃ¡ch thá»©c láº¥y dá»¯ liá»‡uÂ metricÂ cá»§a má»™tÂ Ä‘á»‘i tÆ°á»£ng cáº§n giÃ¡m sÃ¡t, cÃ³ thá»ƒ lÃ  service, node.. ThÃ´ng tin cá»§a job, Ä‘Æ°á»£c cáº¥u hÃ¬nh trong scrape_config cá»§a Prometheus. Má»™t sá»‘ tham sá»‘ cáº¥u hÃ¬nh cá»§a job cÃ³ thá»ƒ khÃ´ng khai bÃ¡o thÃ¬ sáº½ láº¥y máº·c Ä‘á»‹nh á»Ÿ khai bÃ¡o global. Vá»›i bá»™Â kube-prometheus-stackÂ thÃ¬ viá»‡c khai bÃ¡o job trong scrapeConfig Ä‘Æ°á»£c thá»±c hiá»‡n á»Ÿ pháº§n cáº¥u hÃ¬nh trong file helm-value á»Ÿ cáº¥u hÃ¬nh sau:

```
    additionalScrapeConfigs:
    - job_name: minio-job
        scrape_interval: 30s
        scrape_timeout: 5s
        metrics_path: /minio/v2/metrics/cluster
        scheme: http
       static_configs:
       - targets: ['minio.monitoring.svc.cluster.local:9000']

```

Sau Ä‘Ã³ upgrade láº¡i stack Ä‘á»ƒ cáº­p nháº­t giÃ¡ trá»‹ má»›i:

```
helm -n monitoring upgrade prometheus-stack -f values-prometheus-clusterIP.yaml kube-prometheus-stack

```

VÃ  Ä‘á»ƒ cáº¥u hÃ¬nh Ä‘Æ°á»£c prometheus job thÃ¬ báº¡n cáº§n náº¯m rÃµ thÃ´ng tin vá» Ä‘á»‘i tÆ°á»£ng cáº§n láº¥y metric Ä‘á»ƒ khai bÃ¡o cÃ¡c tham sá»‘, quan trá»ng nháº¥t lÃ Â metrics_path, targets

*Vá»›i cáº¥u hÃ¬nh bÃªn trÃªn, mÃ¬nh Ä‘ang thá»±c hiá»‡n láº¥y metric cá»§a service minio vá»›i cÃ¡c thÃ´ng tin nhÆ° sau:*

-   Káº¿t ná»‘i tá»›i serviceÂ minioÂ á»Ÿ namespaceÂ monitoringÂ á»Ÿ port 9000, thÃ´ng tin Ä‘áº§y Ä‘á»§ lÃ Â minio.monitoring.svc.cluster.local:9000
-   Path Ä‘á»ƒ láº¥y metric lÃ Â /minio/v2/metrics/cluster
-   Giao thá»©c láº¥y dá»¯ liá»‡u lÃ Â HTTP
-   Thá»i gian láº¥y dá»¯ liá»‡u lÃ  má»—iÂ 30s
-   Thá»i gian timedout khi láº¥y dá»¯ liá»‡u lÃ Â 5sÂ NhÆ° váº­y má»—i 30s, thÃ¬ prometheus sáº½ gá»i request tá»›iÂ <http://minio.monitoring.svc.cluster.local:9000/minio/v2/metrics/cluster>Â Ä‘á»ƒ láº¥y metric cá»§a service minio.

*NOTE: Vá»›i service cÃ¹ng namespace thÃ¬ cÃ¡c báº¡n cÃ³ thá»ƒ chá»‰ cáº§n cáº¥u hÃ¬nh tÃªn service lÃ  Ä‘á»§ thay vÃ¬ viáº¿t "minio.monitoring.svc.cluster.local" thÃ¬ cÃ³ thá»ƒ chá»‰ cáº§n viáº¿t "minio" cho ngáº¯n gá»n.*

CÃ¡c báº¡n cÅ©ng cÃ³ thá»ƒ tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vá» scrapeConfig trÃªn trang cá»§a Prometheus nhÃ©:Â <https://prometheus.io/docs/alerting/latest/configuration/>

Tá»›i Ä‘Ã¢y, bÃ i toÃ¡n Ä‘áº·t ra lÃ  khi báº¡n cáº§n cáº¥u hÃ¬nh láº¥y metric cá»§a 10 hay 20 hoáº·c nhiá»u hÆ¡n ná»¯a cÃ¡c service, thÃ¬ tÆ°Æ¡ng á»©ng má»—i service báº¡n sáº½ pháº£i cáº¥u hÃ¬nh má»™t job trong scrapeConfig nÃ y. Sáº½ váº«n lÃ  OK náº¿u báº¡n lÃ  má»™t ngÆ°á»i tay to thá»±c sá»± vÃ  ko ngáº¡i viá»‡c khai bÃ¡o thá»§ cÃ´ng, nhÆ°ng cháº¯c cháº¯n náº¿u mÃ¬nh lÃ  manager thÃ¬ sáº½ khÃ´ng Ä‘Ã¡nh giÃ¡ cao viá»‡c nÃ y.

ChÆ°a ká»ƒ tá»›i viá»‡c Ä‘á»ƒ update cÃ¡i scrapeConfig nÃ y cá»§a Prometheus trong bá»™ cÃ i kube-prometheus-stack, báº¡n sáº½ pháº£i update láº¡i file helm-value (nhÆ° trong hÆ°á»›ng dáº«n á»Ÿ pháº§n 8 (<https://viblo.asia/p/k8s-phan-8-monitoring-tren-kubernetes-cluster-dung-prometheus-va-grafana-Qbq5QRkEKD8>) vÃ  sau Ä‘Ã³ pháº£i thá»±c hiá»‡n update láº¡i cáº£ bá»™ kube-prometheus-stack báº±ng lá»‡nh helm upgrade Ä‘á»ƒ cáº­p nháº­t cáº¥u hÃ¬nh má»›i.

NhÆ° váº­y cá»© má»—i láº§n cáº§n cáº¥u hÃ¬nh má»™t job láº¥y dá»¯ liá»‡u cho má»™t service má»›i thÃ¬ báº¡n sáº½ pháº£i cáº­p nháº­t file helm value vÃ  upgrade chart má»™t láº§n.Â Oh my god!! á» ÄÃ‚Y CHÃšNG TÃ”I KHÃ”NG LÃ€M NHÆ¯ Váº¬Y ğŸ˜‚ğŸ˜‚ğŸ˜‚

*ÄÃ³ lÃ  lÃºc báº¡n cáº§n sá»­ dá»¥ng cÃ¡i gá»i lÃ  Service Monitor!!*

Service Monitor
---------------

ServiceMonitor lÃ  má»™t Ä‘á»‘i tÆ°á»£ng cá»§a K8S giÃºp cho Prometheus cÃ³ thá»ƒ tá»± Ä‘á»™ng phÃ¡t hiá»‡n Ä‘Æ°á»£c cÃ¡c Ä‘á»‘i tÆ°á»£ng cáº§n láº¥y metric. NÃ³ sáº½ chá»©a cÃ¡c thÃ´ng tin vá» Ä‘á»‘i tÆ°á»£ng cáº§n giÃ¡m sÃ¡t tÆ°Æ¡ng tá»± nhÆ° cáº¥u hÃ¬nh job trong scrape-config váº­y.

TrÆ°á»›c tiÃªn báº¡n cáº§n hiá»ƒu vá» nguyÃªn lÃ½ hoáº¡t Ä‘á»™ng cá»§a nÃ³ Ä‘Ã£. VÃ  hÃ¬nh dÆ°á»›i Ä‘Ã¢y sáº½ giÃºp báº¡n hiá»ƒu rÃµ hÆ¡n:

![image.png](https://images.viblo.asia/96cb9fe8-b789-49df-bcbf-d49c01c08d16.png)

CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng vá»›i ServiceMonitor:

-   Prometheus sáº½ Ä‘á»c thÃ´ng tin job cá»§a á»Ÿ scrapeConfig vÃ  tá»« cÃ¡c ServiceMonitor, táº¥t cáº£ cÃ¡c Ä‘á»‘i tÆ°á»£ng nÃ y sáº½ hiá»ƒn thá»‹ á»Ÿ pháº§nÂ targetsÂ cá»§a Prometheus
    -   Viá»‡c Ä‘á»c tá»« scrapeConfig thÃ¬ lÃ  quÃ¡ rÃµ rÃ ng rá»“i, nÃ³ Ä‘Æ°á»£c cáº¥u hÃ¬nh á»Ÿ file config cá»§a Prometheus
    -   Viá»‡c Ä‘á»c ServiceMonitor thÃ¬ dá»±a vÃ o cáº¥u hÃ¬nh cá»§a Prometheus gá»“m 2 tham sá»‘:
        -   serviceMonitorNamespaceSelector: LÃ  cáº¥u hÃ¬nh chá»‰ Ä‘á»c cÃ¡c ServiceMonitor á»Ÿ má»™t sá»‘ namespace nháº¥t Ä‘á»‹nh, máº·c Ä‘á»‹nh lÃ  Ä‘á»c tá»« táº¥t cáº£ cÃ¡c namespace
        -   serviceMonitorSelector: LÃ  cáº¥u hÃ¬nh vá» cÃ¡ch chá»n cÃ¡c ServiceMonitor sáº½ Ä‘Æ°á»£c Ä‘á»c Ä‘á»ƒ lÆ°u cáº¥u hÃ¬nh job cho Prometheus. CÃ¡c báº¡n cÃ³ thá»ƒ cáº¥u hÃ¬nh theo nhiá»u rule khÃ¡c nhau, mÃ¬nh chá»n cÃ¡ch cáº¥u hÃ¬nh theoÂ labelÂ cá»§aÂ ServiceMonitorÂ lÃ  cÃ¡ch Ä‘Æ¡n giáº£n vÃ  dá»… dÃ¹ng nháº¥t.
-   Trong cáº¥u hÃ¬nh cá»§a ServiceMonitor sáº½ cÃ³ thÃ´ng tinÂ labelÂ cá»§a nÃ³ (dÃ¹ng cho Prometheus select nhÆ° trÃªn) vÃ  cÃ¡c thÃ´ng tin tá»›i Ä‘á»‘i tÆ°á»£ng cáº§n giÃ¡m sÃ¡t (giá»‘ng cÃ¡c thÃ´ng tin khai bÃ¡o cho job trong scrapeConfig váº­y)

NhÆ° váº­y trÆ°á»›c tiÃªn ta cáº§n cáº¥u hÃ¬nh láº¡i Prometheus Ä‘á»ƒ update láº¡i cáº¥u hÃ¬nh ServiceMonitor selector. á» Ä‘Ã¢y mÃ¬nh sáº½ select tá»« táº¥t cáº£ namespace, vÃ  sáº½ filter cÃ¡c ServiceMonitor cÃ³ gÃ¡n label**Â [app.kubernetes.io/instance**](http://app.kubernetes.io/instance**)Â tÆ°Æ¡ng á»©ng vá»›i giÃ¡ trá»‹ mÃ¬nh sáº½ set cho cÃ¡c serviceMonitor khai bÃ¡o sau nÃ y lÃ Â viettq-service-monitor.

MÃ¬nh sáº½ sá»­a láº¡i tham sá»‘ "serviceMonitorSelector:" trong helm-value cá»§a kube-prometheus-stack (fileÂ values-prometheus-clusterIP.yaml) nhÆ° sau:

```
    serviceMonitorSelector:
      matchExpressions:
      - key: app.kubernetes.io/instance
        operator: In
        values:
          - viettq-service-monitor
          - nginx-ingress-controller
          - prometheus-stack

```

sau Ä‘Ã³ upgrade láº¡i stack Ä‘á»ƒ cáº­p nháº­t giÃ¡ trá»‹ má»›i:

```
helm -n monitoring upgrade prometheus-stack -f values-prometheus-clusterIP.yaml kube-prometheus-stack

```

NhÆ° váº­y táº¥t cáº£ cÃ¡c serviceMonitor á»Ÿ táº¥t cáº£ cÃ¡c namespace náº¿u cÃ³ gÃ¡n labelÂ [app.kubernetes.io/instance](http://app.kubernetes.io/instance)Â vá»›i 1 trong cÃ¡c giÃ¡ trá»‹ nhÆ° trÃªn thÃ¬ sáº½ Ä‘Æ°á»£c Prometheus Ä‘á»c vÃ o náº¡p vÃ o cáº¥u hÃ¬nh cá»§a nÃ³.

*Äáº¿n Ä‘Ã¢y thÃ¬ viá»‡c giÃ¡m sÃ¡t thÃªm 10 hay 20 service má»›i Ä‘Ã£ khÃ´ng cÃ²n lÃ  váº¥n Ä‘á» ná»¯a. Báº¡n chá»‰ cáº§n thá»‘ng nháº¥t 1 label sáº½ gÃ¡n cho toÃ n bá»™ cÃ¡c serviceMonitor má»›i sau nÃ y trÃ¹ng vá»›i cáº¥u hÃ¬nh serviceMonitorSelector cá»§a Prometheus lÃ  nÃ³ sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c Ä‘á»c vÃ  khÃ´ng cáº§n pháº£i update gÃ¬ vá»›i Prometheus ná»¯a cáº£! NgoÃ i ra, vá»›i cÃ¡ch tiáº¿p cáº­n nÃ y, má»—i service cáº§n giÃ¡m sÃ¡t báº¡n sáº½ táº¡o má»™t file serviceMonitor dáº¡ng yaml sáº½ ráº¥t tiá»‡n lá»£i cho viá»‡c quáº£n lÃ½, cáº­p nháº­t vÃ  tÃ¡i sá»­ dá»¥ng sau nÃ y.*

ÄÃ¢y lÃ  cáº¥u hÃ¬nh máº«u cá»§a ServiceMonitor Ä‘á»ƒ giÃ¡m sÃ¡t service minio, tÆ°Æ¡ng tá»± vá»›i cáº¥u hÃ¬nh scrapeConfig mÃ¬nh giá»›i thiá»‡u bÃªn trÃªn. Báº¡n táº¡o file serviceMonitor-minio.yaml vá»›i ná»™i dung nhÆ° sau:

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: minio-monitor
  # Change this to the namespace the Prometheus instance is running in
  namespace: monitoring
  labels:
    app.kubernetes.io/instance: viettq-service-monitor
spec:
  endpoints:
  - port: 9000
    interval: 15s
    scheme: http
    metrics_path: /minio/v2/metrics/cluster
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
      app.kubernetes.io/instance: minio

```

VÃ  apply nÃ³ vÃ o k8s vÃ  kiá»ƒm tra káº¿t quáº£:

```
[sysadmin@vtq-cicd prometheus]$ kubectl apply -f serviceMonitor-minio.yaml
[sysadmin@vtq-cicd prometheus]$ kubectl -n monitoring get servicemonitors.monitoring.coreos.com -l "app.kubernetes.io/instance=viettq-service-monitor"
NAME                                 AGE
fluentd                              6d17h
longhorn-prometheus-servicemonitor   21d
minio-monitor                        21d
redis-cluster                        18d

```

Vá»›i cáº¥u hÃ¬nh trÃªn thÃ¬ Ä‘á»‘i tÆ°á»£ng serviceMonitor cÃ³ tÃªn lÃ Â minio-monitorÂ sáº½ Ä‘Æ°á»£c deploy vÃ o namespace lÃ Â monitoring, vÃ  cÃ³ gÃ¡n key "[app.kubernetes.io/instance:](http://app.kubernetes.io/instance:)Â viettq-service-monitor"Â thá»a mÃ£n Ä‘iá»u kiá»‡n trong cáº¥u hÃ¬nh serviceMonitorSelector cá»§a Prometheus nÃªn nÃ³ sáº½ Ä‘Æ°á»£c Prometheus náº¡p vÃ o cáº¥u hÃ¬nh cá»§a nÃ³.

Tiáº¿p Ä‘áº¿n, trong cáº¥u hÃ¬nh cá»§a serviceMonitor nÃ y sáº½ cÃ³ pháº§n cáº¥u hÃ¬nhÂ specÂ chÃ­nh lÃ  cáº¥u hÃ¬nh thÃ´ng tin láº¥y metric tá»« Ä‘á»‘i tÆ°á»£ng cáº§n giÃ¡m sÃ¡t, gá»“m thÃ´ng tinÂ endpointsÂ (port/metric_path) vÃ  thÃ´ng tin object cáº§n giÃ¡m sÃ¡t (namespaceSelectorÂ vÃ Â selector)

Äá»ƒ lÃ m rÃµ chá»— nÃ y, cÃ¡c báº¡n thá»±c hiá»‡n lá»‡nh get service theo thÃ´ng tin selector bÃªn trÃªn trong cáº¥u hÃ¬nh serviceMonitor sáº½ rÃµ. NÃ³ sáº½ tÃ¬m Ä‘áº¿n service á»Ÿ namespace lÃ Â monitoringÂ cÃ³ gÃ¡n labelÂ [app.kubernetes.io/instance=minio](http://app.kubernetes.io/instance=minio):

```
[sysadmin@vtq-cicd prometheus]$ kubectl -n monitoring get service -l "app.kubernetes.io/instance=minio"
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
minio   ClusterIP   10.233.19.122   <none>        9000/TCP,9001/TCP   21d

```

*Chá»— nÃ y cÃ¡c báº¡n cáº§n lÆ°u Ã½ vá» Ä‘á»‘i tÆ°á»£ng giÃ¡m sÃ¡t cÃ³ 2 loáº¡i: ChÃ­nh nÃ³ há»— trá»£ metric hay pháº£i dÃ¹ng exporter. Trong trÆ°á»ng há»£p nÃ y minio há»— trá»£ sáºµn metric nÃªn ta sáº½ káº¿t ná»‘i tá»›i service minio á»Ÿ port vÃ  path mÃ  nÃ³ cung cáº¥p metric.*

*Vá»›i trÆ°á»ng há»£p service khÃ´ng há»— trá»£ metric mÃ  pháº£i dÃ¹ng exporter thÃ¬ ta sáº½ cáº§n cáº¥u hÃ¬nh pháº§n endpoints vÃ  selector bÃªn trÃªn tá»›i thÃ´ng tin service exporter.*

CÃ i service há»— trá»£ sáºµn cáº£ metric vÃ  serviceMonitor (redis)
----------------------------------------------------------

CÃ³ má»™t Ä‘iá»u ráº¥t tuyá»‡t vá»i lÃ  nhiá»u open-source cÃ i qua helm-chart ngoÃ i há»— trá»£ sáºµn metric cÃ²n há»— trá»£ luÃ´n cáº£ template táº¡o serviceMonitor vÃ  PrometheusRule luÃ´n. á» Ä‘Ã¢y mÃ¬nh sáº½ cÃ i thá»­Â redisÂ báº±ng helm lÃ  má»™t opensource mÃ  mÃ¬nh tháº¥y há»— trá»£ táº¥t cáº£ nhá»¯ng thá»© mÃ¬nh nÃ³i bÃªn trÃªn. CÃ¡ch cÃ i má»™t pháº§n má»m dÃ¹ng helm thÃ¬ mÃ¬nh Ä‘Ã£ mÃ´ táº£ nhiá»u á»Ÿ cÃ¡c pháº§n trÆ°á»›c rá»“i nÃªn á»Ÿ Ä‘Ã¢y mÃ¬nh nÃ³i nhanh vÃ  tÃ³m táº¯t thÃ´i.

```
mkdir -p /home/sysadmin/open-sources
cd /home/sysadmin/open-sources
mkdir redis
cd redis
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm search repo redis
helm pull bitnami/redis-cluster --version 7.5.1
ls -lrt
tar -xzf redis-cluster-7.5.1.tgz
cp redis-cluster/values.yaml values-redis.yaml
vi values-redis.yaml

```

MÃ¬nh sáº½ cÃ i redis lÃªn namespaceÂ prodÂ sá»­ dá»¥ng háº§u háº¿t cáº¥u hÃ¬nh máº·c Ä‘á»‹nh, chá»‰ sá»­a má»™t vÃ i tham sá»‘ nhÆ° sau Ä‘á»ƒ enableÂ metricÂ vÃ Â serviceMonitor:

```
usePassword: false
redis: "30101"
type: NodePort
storageClass: "longhorn-storage-delete"
size: 1Gi

metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    namespace: "monitoring"
    labels:
      app.kubernetes.io/instance: viettq-service-monitor

```

Sau khi cáº­p nháº­t cÃ¡c giÃ¡ trá»‹ trong fileÂ values-redis.yamlÂ nhÆ° bÃªn trÃªn thÃ¬ thá»±c hiá»‡n cÃ i Ä‘áº·t:

```
[sysadmin@vtq-cicd redis]$ helm -n prod install redis-cluster -f values-redis.yaml redis-cluster
[sysadmin@vtq-cicd redis]$ kubectl -n prod get pods -l " app.kubernetes.io/instance=redis-cluster"
NAME              READY   STATUS    RESTARTS   AGE
redis-cluster-0   2/2     Running   0          19d
redis-cluster-1   2/2     Running   0          19d
redis-cluster-2   2/2     Running   0          19d
redis-cluster-3   2/2     Running   0          19d
redis-cluster-4   2/2     Running   0          19d
redis-cluster-5   2/2     Running   0          19d
[sysadmin@vtq-cicd redis]$ kubectl -n prod get service -l " app.kubernetes.io/instance=redis-cluster"
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE
redis-cluster            NodePort    10.233.63.22   <none>        6379:30101/TCP       19d
redis-cluster-headless   ClusterIP   None           <none>        6379/TCP,16379/TCP   19d
redis-cluster-metrics    ClusterIP   10.233.49.30   <none>        9121/TCP             19d
[sysadmin@vtq-cicd redis]$ kubectl -n monitoring get servicemonitors.monitoring.coreos.com redis-cluster
NAME            AGE
redis-cluster   19d

```

NhÆ° váº­y sau khi cÃ i Ä‘áº·t ta cÃ³ má»™t cá»¥m redis-cluster gá»“m 6 pods, 3 service á»Ÿ namespaceÂ prodÂ vÃ  má»™t object serviceMonitor lÃ Â redis-clusterÂ Ä‘Æ°á»£c táº¡o á»Ÿ namespaceÂ monitoringÂ (theo Ä‘Ãºng nhá»¯ng gÃ¬ ta cáº¥u hÃ¬nh á»Ÿ fileÂ values-redis.yaml

Giá» ta kiá»ƒm tra xem tháº±ng serviceMonitor cá»§a redis nÃ y cÃ³ gÃ¬:

```
[sysadmin@vtq-cicd redis]$ kubectl -n monitoring describe servicemonitors.monitoring.coreos.com redis-cluster
Name:         redis-cluster
Namespace:    monitoring
Labels:       app.kubernetes.io/instance=viettq-service-monitor
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=redis-cluster
              helm.sh/chart=redis-cluster-7.5.1
Annotations:  meta.helm.sh/release-name: redis-cluster
              meta.helm.sh/release-namespace: prod
API Version:  monitoring.coreos.com/v1
Kind:         ServiceMonitor
Metadata:
  Creation Timestamp:  2022-04-29T03:27:03Z
  Generation:          1
  Managed Fields:
    API Version:  monitoring.coreos.com/v1
    Fields Type:  FieldsV1
   <some information truncated>
    Manager:         helm
    Operation:       Update
    Time:            2022-04-29T03:27:03Z
  Resource Version:  5816043
  Self Link:         /apis/monitoring.coreos.com/v1/namespaces/monitoring/servicemonitors/redis-cluster
  UID:               bafc11b4-e814-4968-ad0d-2782184c6515
Spec:
  Endpoints:
    Port:  metrics
  Namespace Selector:
    Match Names:
      prod
  Selector:
    Match Labels:
      app.kubernetes.io/component:  metrics
      app.kubernetes.io/instance:   redis-cluster
      app.kubernetes.io/name:       redis-cluster
Events:

```

Trong Ä‘Ã³ ta tháº¥y má»™t vÃ i thÃ´ng tin quan trá»ng nhÆ°:

-   "Labels:Â [app.kubernetes.io/instance=viettq-service-monitor](http://app.kubernetes.io/instance=viettq-service-monitor)" --> ÄÃ¢y chÃ­nh lÃ  thÃ´ng tin Label Ä‘á»ƒ Prometheus lá»±a chá»n Ä‘á»c thÃ´ng tin tá»« serviceMonitor nÃ y (nhÆ° Ä‘Ã£ mÃ´ táº£ bÃªn trÃªn). Vá»›i má»—i serviceMonitor má»›i ta sáº½ Ä‘á»u thÃªm label nÃ y vÃ o Ä‘á»ƒ Ä‘Æ°á»£c Ä‘á»c tá»± Ä‘á»™ng vÃ o Prometheus.
-   ThÃ´ng tin spec: LÃ  thÃ´ng tin tá»›i Ä‘á»‘i tÆ°á»£ng cáº§n láº¥y metric. á» Ä‘Ã¢y nÃ³ tá»± hiá»ƒu cáº§n káº¿t ná»‘i tá»›i service á»Ÿ namespaceÂ prodÂ mÃ  cÃ³ gÃ¡n cÃ¡c Label nhÆ° bÃªn dÆ°á»›i, á»Ÿ port tÃªn lÃ  metrics.

```
Spec:
  Endpoints:
    Port:  metrics
  Namespace Selector:
    Match Names:
      prod
  Selector:
    Match Labels:
      app.kubernetes.io/component:  metrics
      app.kubernetes.io/instance:   redis-cluster
      app.kubernetes.io/name:       redis-cluster

```

Verify láº¡i thÃ´ng tin service Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh theo spec trÃªn báº±ng lá»‡nh:

```
[sysadmin@vtq-cicd redis]$ kubectl -n prod get service -l "app.kubernetes.io/component=metrics" -l "app.kubernetes.io/instance=redis-cluster" -l "app.kubernetes.io/name=redis-cluster"
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE
redis-cluster            NodePort    10.233.63.22   <none>        6379:30101/TCP       19d
redis-cluster-headless   ClusterIP   None           <none>        6379/TCP,16379/TCP   19d
redis-cluster-metrics    ClusterIP   10.233.49.30   <none>        9121/TCP             19d

```

Quay láº¡i trang Prometheus xem service nÃ y Ä‘Ã£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong pháº§n targets hay chÆ°a:

![image.png](https://images.viblo.asia/fb38124e-0124-43ed-83ca-23131d2810b6.png)

NhÆ° váº­y lÃ  pháº§n láº¥y metric cá»§a Redis Ä‘Ã£ ok rá»“i. Tháº­t sá»± lÃ  Ä‘Æ¡n giáº£n vÃ  nhanh gá»n pháº£i khÃ´ng cÃ¡c báº¡nÂ ![ğŸ˜ƒ](https://twemoji.maxcdn.com/v/14.0.2/72x72/1f603.png)

Tiáº¿p Ä‘áº¿n lÃ  chÃºt mÃ u mÃ¨ cho nÃ³ báº±ng cÃ¡ch add redis-dashboard trÃªn Grafana (cÃ¡i nÃ y cÃ¡c báº¡n tá»± tÃ¬m trÃªn máº¡ng nhÃ©, mÃ¬nh lÃ m lÃ¢u quÃªn máº¥t ID cá»§a dashboard rá»“i).

VÃ  Ä‘Ã¢y lÃ  thÃ nh quáº£ Ä‘á»ƒ tiáº¿p tá»¥c up facebook Ä‘Ã¢y:

![image.png](https://images.viblo.asia/30aafe0c-543f-4db5-903b-d283449bca73.png)

VÃ  cÅ©ng hoÃ n toÃ n tÆ°Æ¡ng tá»± vá»›i redis, khi cÃ i Ä‘áº·t NGINX-Ingress controller thÃ¬ cÃ¡c báº¡n cÅ©ng hoÃ n toÃ n cÃ³ thá»ƒ enableÂ metricÂ vÃ Â serviceMonitorÂ cho nÃ³ (cÃ¡c báº¡n tá»± voá»c fileÂ helm-valueÂ cá»§a nÃ³ nhÃ©) sau Ä‘Ã³ cÅ©ng download grafana-dashboard cho nÃ³ Ä‘á»ƒ hiá»ƒn thá»‹ sáº½ Ä‘Æ°á»£c káº¿t quáº£ long lanh nhÆ° tháº¿ nÃ y:

![image.png](https://images.viblo.asia/2940099a-b444-499f-a5a7-803b553a6127.png)

Bonus
-----

ThÃªm má»™t thÃ´ng tin ná»¯a náº¿u báº¡n nÃ o cÃ i Ä‘áº·t Longhorn storage cho K8S cÃ³ thá»ƒ sáº½ gáº·p pháº£i váº¥n Ä‘á» cáº¥u hÃ¬nh scrapeConfig xong thÃ¬ nÃ³ sáº½ chá»‰ láº¥y Ä‘Æ°á»£c thÃ´ng tin cá»§a 1 node (trong N node mÃ  báº¡n cÃ³). Muá»‘n láº¥y metric cá»§a Longhorn thÃ¬ báº¯t buá»™c pháº£i sá»­ dá»¥ng qua serviceMonitor Ä‘á»ƒ nÃ³ láº¥y Ä‘á»§ thÃ´ng tin tá»« táº¥t cáº£ cÃ¡c node nhÃ©! CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o cáº¥u hÃ¬nh serviceMonitor cá»§a Longhorn nhÆ° sau nhÃ©, tÃ¹y biáº¿n theo mÃ´i trÆ°á»ng cá»§a cÃ¡c báº¡n ná»¯a lÃ  ok:

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: longhorn-prometheus-servicemonitor
  namespace: monitoring
  labels:
    name: longhorn-prometheus-servicemonitor
    release:  prometheus-grafana-stack
    k8s-app: longhorn
    app: kube-prometheus-stack
    app.kubernetes.io/instance: viettq-service-monitor
spec:
  selector:
    matchLabels:
      app: longhorn-manager
  namespaceSelector:
    matchNames:
    - storage
  endpoints:
  - port: manager

```

Sau Ä‘Ã³ báº¡n tÃ¬m grafana-dashboard cho nÃ³ nhÃ© (keyword: longhorn example v1.1.0) Ä‘á»ƒ hiá»ƒn thá»‹ thÃ´ng tin lÃªn dashboard:

![image.png](https://images.viblo.asia/dd8fb491-107e-46e0-a099-e4c24bf201b1.png)



Tham kháº£o:
- https://viblo.asia/p/k8s-phan-9-service-monitor-cach-giam-sat-ung-dung-tren-prometheus-mot-cach-nhanh-chong-va-hieu-qua-1Je5EAry5nL
